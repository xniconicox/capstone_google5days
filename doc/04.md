ã„ã„æ„Ÿã˜ã«ç’°å¢ƒãã‚ã£ã¦ã¾ã™ ğŸ‘
ä»Šã®çŠ¶æ…‹ã‚’å‰æã«ã€

* **pyproject.tomlï¼ˆæœ€å°ä¾å­˜ã«ä¿®æ­£ï¼‰**
* **README.md**
* **doc/TESTING.md**
* **doc/KAGGLE_BUILD_AND_TEST.md**

ã‚’ã€Œã„ã¾ã® uv + .venv ã®å®Ÿç’°å¢ƒã«åˆã†å½¢ã€ã§ã¾ã¨ã‚ç›´ã—ã¾ã™ã€‚

---

## 1. pyproject.tomlï¼ˆæœ€å°é™ã®ä¾å­˜ã«ä¿®æ­£æ¡ˆï¼‰

ã€Œå‹•ã‹ã™ã®ã«æœ¬å½“ã«å¿…è¦ãªãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ä¾å­˜ã€ã ã‘ã«çµã‚‹ã¨ã€ã ã„ãŸã„ã“ã‚“ãªæ„Ÿã˜ã«ãªã‚Šã¾ã™ï¼š

* ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒç›´æ¥ import ã—ã¦ã„ãã†ãªã‚‚ã®

  * `google-adk`ï¼ˆADK æœ¬ä½“ï¼‰
  * `python-dotenv`ï¼ˆ.env ãƒ­ãƒ¼ãƒ‰ï¼‰
  * `pandas`ï¼ˆçµæœæ•´å½¢ï¼‰
  * `requests`ï¼ˆSTAC APIï¼‰
  * `tabulate`ï¼ˆè¡¨å½¢å¼ã®å‡ºåŠ›ï¼‰
  * `shapely`ï¼ˆAOI ã®ã‚¸ã‚ªãƒ¡ãƒˆãƒªï¼‰
* ãƒ†ã‚¹ãƒˆç”¨: `pytest` ã‚’ optional ã«

`pyproject.toml` ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«æ›¸ãæ›ãˆã‚‹ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ã™ã€‚

```toml
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "capstone"
version = "0.1.0"
description = "Satellite Metadata Search Agent (STAC + Google ADK)"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "google-adk>=1.19.0",
    "python-dotenv>=1.2.1",
    "pandas>=2.3.3",
    "requests>=2.32.5",
    "tabulate>=0.9.0",
    "shapely>=2.1.2",
]

[project.optional-dependencies]
test = [
    "pytest>=9.0.1",
]

[tool.setuptools]
package-dir = {"" = "src"}

[tool.setuptools.packages.find]
where = ["src"]

[tool.setuptools.package-data]
"capstone.aoi" = ["aoi_catalog.json"]
```

â€» `agents.satellite_stac_agent` ã® package-data ã¯ã‚‚ã†ä½¿ã£ã¦ã„ãªã„ã®ã§ã€
`capstone.aoi` ã«ä»˜ã‘æ›¿ãˆã¦ã„ã¾ã™ï¼ˆ`src/capstone/aoi/aoi_catalog.json` ç”¨ï¼‰ã€‚

---

## 2. README.mdï¼ˆæœ€æ–°ç’°å¢ƒã«åˆã‚ã›ãŸç‰ˆï¼‰

ä»¥ä¸‹ã‚’ `README.md` ã«ç½®ãæ›ãˆã‚‹æƒ³å®šã§ã™ã€‚

````markdown
# Satellite Metadata Search Agent

Natural-language interface for searching satellite imagery metadata using **STAC** and the **Google Agent Development Kit (ADK)**.

This repository is my submission for the **â€œAgents Intensive Capstone Projectâ€** Kaggle competition.

---

## 1. Overview

Searching satellite imagery normally requires detailed knowledge of:

- Bounding boxes and polygon AOIs
- Date ranges and seasonal interpretation
- Cloud cover thresholds
- STAC API structure (collections, filters, pagination)

This project implements an **LLM-based agent** that abstracts all of that.  
Users can ask in natural language, and the agent:

1. Resolves location text into a **bounding box (AOI)**
2. Interprets **time ranges** and **cloud constraints**
3. Calls a real **STAC API** (Sentinel-2 L2A)
4. Returns a **structured result table** with relevant scenes

The design focuses on:

- Deterministic tool usage
- Production-aligned prompting
- Full ADK evaluation (normal / clarification / boundary cases)

---

## 2. Repository structure

```text
capstone/
â”œâ”€â”€ README.md
â”œâ”€â”€ README_jp.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ uv.lock
â”œâ”€â”€ src/
â”‚   â””â”€â”€ capstone/
â”‚       â”œâ”€â”€ agent/
â”‚       â”‚   â”œâ”€â”€ prompts.py
â”‚       â”‚   â””â”€â”€ stac_agent_adk.py
â”‚       â”œâ”€â”€ aoi/
â”‚       â”‚   â”œâ”€â”€ aoi_catalog.json
â”‚       â”‚   â””â”€â”€ aoi_catalog.py
â”‚       â”œâ”€â”€ tools/
â”‚       â”‚   â””â”€â”€ stac_search.py
â”‚       â””â”€â”€ scripts/
â”‚           â”œâ”€â”€ run_eval.py
â”‚           â””â”€â”€ eval/
â”‚               â”œâ”€â”€ agent.py
â”‚               â”œâ”€â”€ normal.evalset.json
â”‚               â”œâ”€â”€ clarification.evalset.json
â”‚               â””â”€â”€ boundary.evalset.json
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ unit/
â”‚       â”œâ”€â”€ test_aoi_catalog.py
â”‚       â”œâ”€â”€ test_adk_eval.py
â”‚       â””â”€â”€ adk_test_cases.txt
â””â”€â”€ doc/
    â”œâ”€â”€ TESTING.md
    â”œâ”€â”€ KAGGLE_BUILD_AND_TEST.md
    â””â”€â”€ è¡›æ˜Ÿãƒ‡ãƒ¼ã‚¿ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ·±æ˜ã‚Šèª¿æŸ».pdf
````

Archived directories (`archive_old`, `agents_old`) are early experiments and not used in the final agent.

---

## 3. Authentication Setup (Gemini API)

The project uses the Gemini API via ADK.

You need:

1. A Gemini API key
2. A `.env` file in the project root

### Steps

```bash
cp .env.example .env
```

Edit:

```env
GOOGLE_API_KEY=YOUR_KEY
```

Load:

```bash
set -a
source .env
set +a
```

No `gcloud` login or ADC credentials are required;
ADK uses `GOOGLE_API_KEY` from the environment.

---

## 4. Running the agent

All commands assume the project root:

```bash
cd /home/kimura/work/google5days/capstone
```

and a local `.venv` managed by `uv`.

### 4.1 Install dependencies

```bash
uv sync
```

This creates `.venv/` and installs dependencies from `pyproject.toml`.

---

### 4.2 Interactive mode (ADK Web UI)

Launch ADK Web UI with the scripts directory:

```bash
adk web src/capstone/scripts/
```

Then:

1. Open the printed URL (for example `http://localhost:8000`) in your browser
2. In the left panel, select the **eval agent** defined in `src/capstone/scripts/eval/agent.py`
3. Click **â€œStart Chatâ€** and begin chatting with the agent

This lets you:

* Try natural-language queries
* Inspect clarification behavior
* Observe tool calls and final outputs

---

### 4.3 Programmatic eval (fast smoke test)

```bash
.venv/bin/python -m capstone.scripts.run_eval
```

This runs the golden-case evals (normal / clarification / boundary) via ADK and prints a summary.

---

### 4.4 Full evaluation (detailed metrics)

For full CLI evaluation (with detailed metrics and logs), see **doc/TESTING.md**.

---

## 5. Testing & Kaggle

* For local and CI testing: see **doc/TESTING.md**
* For Kaggle build & test instructions: see **doc/KAGGLE_BUILD_AND_TEST.md**

````

---

## 3. doc/TESTING.mdï¼ˆæœ€æ–°ç‰ˆï¼‰

```markdown
# Testing Guide  
Satellite Metadata Search Agent

This document summarizes how to test the Satellite Metadata Search Agent.  
The workflow follows ADKâ€™s recommended evaluation structure and supports development, CI, and production release.

---

## 1. Unit Tests (pytest)

Run deterministic tests:

```bash
.venv/bin/pytest tests/unit -q
````

Validates:

* AOI catalog behavior
* Time / cloud-cover parsing logic
* Basic ADK wiring

---

## 2. Programmatic ADK Evaluation (CI Smoke Test)

Quick validation using `AgentEvaluator`:

```bash
.venv/bin/python -m capstone.scripts.run_eval
```

Confirms:

* Evalset files load correctly
* Agent imports correctly
* No fatal runtime errors

Used for CI and local development.

---

## 3. Manual Evaluation (ADK Web UI)

Launch the interactive UI:

```bash
adk web src/capstone/scripts/
```

Then:

1. Open the printed URL in your browser
2. Select the **Eval** tab
3. Choose the configured eval agent
4. Start a chat or run specific eval cases

Check:

* Clarification behavior
* Cloud-cover mapping
* Correct tool flow
* Reasonable final output formatting

Use this during prompt tuning and debugging.

---

## 4. Full ADK CLI Evaluation (Detailed Metrics)

Run complete evaluation:

```bash
adk eval src/capstone/scripts/eval \
    src/capstone/scripts/eval/normal.evalset.json \
    src/capstone/scripts/eval/clarification.evalset.json \
    src/capstone/scripts/eval/boundary.evalset.json \
  --config_file_path=src/capstone/scripts/eval/test_config.json \
  --print_detailed_results > src/capstone/scripts/eval/result.txt
```

Outputs:

* Rubric scores
* `response_match_score`
* Tool trajectory comparison
* Eval history files

Required before PR merge and release.

---

## 5. Recommended Workflow

### Local Development

* `.venv/bin/pytest`
* `adk web src/capstone/scripts/` for interactive testing
* Optional: `.venv/bin/python -m capstone.scripts.run_eval`

### CI (Push)

* `.venv/bin/pytest`
* `.venv/bin/python -m capstone.scripts.run_eval`

### Pull Request (Quality Gate)

* Full ADK CLI evaluation
* Confirm **all cases pass**
* Reviewer checks `result.txt`

### Release / Production

* Full ADK evaluation on `main`
* Ensure no regressions with the production model
* Deploy

---

This layered workflow ensures:

* Fast feedback
* Reliable safety checks
* High-confidence releases

````

---

## 4. doc/KAGGLE_BUILD_AND_TEST.mdï¼ˆuv + .venv å‰æã§ç¢ºå®šç‰ˆï¼‰

```markdown
# Kaggle Build & Test Guide  
Satellite Metadata Search Agent

This document describes exactly how a Kaggle reviewer can build and test this project locally.

Assumptions:

- Standard Python 3.11+ environment
- Repository cloned locally
- `uv` is available
- Gemini API key is configured as described in `README.md` (for ADK-based tests)

Covers:

1. Clone & environment setup  
2. Install dependencies with `uv`  
3. Run unit tests  
4. Run ADK eval tests (golden cases)  
5. Optional: ADK Web UI  
6. Project structure reference  
7. TL;DR  

---

## 1. Clone

```bash
git clone <REPO_URL>
cd capstone
````

---

## 2. Environment Setup (uv + .venv)

Install dependencies and create `.venv`:

```bash
uv sync
```

This:

* Creates `.venv/` in the project root
* Installs dependencies from `pyproject.toml`

(You may optionally `source .venv/bin/activate`,
but commands below use `.venv/bin/...` explicitly.)

---

## 3. Unit Tests

Run Python unit tests:

```bash
.venv/bin/pytest tests/unit
```

Ensures:

* AOI resolution (`aoi_catalog`) works correctly
* Core utilities behave deterministically

---

## 4. ADK Eval Tests (Golden Cases)

Run all evalsets through the helper script:

```bash
.venv/bin/python -m capstone.scripts.run_eval
```

This:

* Loads `normal.evalset.json`, `clarification.evalset.json`, and `boundary.evalset.json`
* Evaluates the ADK agent against the golden cases
* Prints an aggregated summary to stdout

---

## 5. Optional: ADK Web UI

You can also test interactively via ADK Web UI.

Start the UI:

```bash
adk web src/capstone/scripts/
```

Then:

1. Open the printed URL (example: `http://localhost:8000`)
2. Select the eval agent / Eval tab
3. Chat with the agent using natural-language queries such as:

   * â€œFind Sentinel-2 images over eastern Hokkaido between 2023-06-15 and 2023-06-30 with less than 20% cloud cover.â€
   * â€œShow me almost cloud-free images over Japan in 2023.â€
   * â€œFind cloud-free images around Sapporo in September.â€

     * â†’ The agent should ask for a more precise date range.

The behavior should match the golden-case evals.

---

## 6. Project Structure (Quick Reference)

```text
src/capstone/
  agent/
    prompts.py
    stac_agent_adk.py
  aoi/
    aoi_catalog.json
    aoi_catalog.py
  tools/
    stac_search.py
  scripts/
    run_eval.py
    eval/
      agent.py
      normal.evalset.json
      clarification.evalset.json
      boundary.evalset.json

tests/
  unit/
    test_aoi_catalog.py
    test_adk_eval.py
```

Archived directories are unused.

---

## 7. TL;DR

```bash
# 1. Clone
git clone <REPO_URL>
cd capstone

# 2. Install deps (uv + .venv)
uv sync

# 3. Run unit tests
.venv/bin/pytest tests/unit

# 4. Run golden-case evals
.venv/bin/python -m capstone.scripts.run_eval

# 5. (Optional) Launch ADK Web UI
adk web src/capstone/scripts/
```

This completes the build & test process for a Kaggle reviewer using `uv` and the bundled `.venv`.

````

---

ã‚ã¨ã¯ï¼š

```bash
# pyproject ã‚’ç·¨é›†ã—ãŸã‚‰
uv lock
git add pyproject.toml uv.lock README.md doc/TESTING.md doc/KAGGLE_BUILD_AND_TEST.md
git commit -m "Align docs and dependencies with uv environment"
````

ã¿ãŸã„ãªæµã‚Œã§ã‚³ãƒŸãƒƒãƒˆã™ã‚Œã°ã€
ã‚³ãƒ¼ãƒ‰ãƒ»ç’°å¢ƒãƒ»ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒãã‚Œã„ã«æƒã£ãŸçŠ¶æ…‹ã«ãªã‚Šã¾ã™ã€‚
